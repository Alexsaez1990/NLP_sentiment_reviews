{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Carga datos**"
      ],
      "metadata": {
        "id": "sJh1bB0DQY4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cPi3eoFjB9ex"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_balanced_sent = pd.read_csv('/content/drive/MyDrive/NLP/df_balanced_sent.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_balanced_sent.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shmJohZPCLOd",
        "outputId": "0eea0999-54b8-4a0d-9aaf-a986ae355db7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          reviewText  sentiment\n",
            "0  It's got a lot more kick than what you'll find...          1\n",
            "1  Cookies seemed a little old, did not come with...          0\n",
            "2  These caramels are grainy and tasteless. They ...          0\n",
            "3                                 very good , strong          1\n",
            "4  Not as good as home made from scratch, but not...          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Función de preprocesado completa**"
      ],
      "metadata": {
        "id": "wwgegwbuQd_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V55gLuPEnlA",
        "outputId": "2e280c6b-ded0-4755-df0d-cfc0e670bfe4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqXkCrGNEqp-",
        "outputId": "21aa3fb6-196d-4a74-a3a3-70b4d4cb0527"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_juv-U3NYmg",
        "outputId": "c90293cf-e06a-4f32-ffb4-573718200e92"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.12)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "from num2words import num2words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import FreqDist\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "  # Eliminamos acentos y signos de puntuación\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "  # Tokenizamos y convertimos a minúsculas\n",
        "  #tokenizer = RegexpTokenizer('r\\w+')\n",
        "  tokenized_text = word_tokenize(text.lower())\n",
        "\n",
        "  # Eliminamos signos de puntuación y caracteres especiales\n",
        "  tokenized_text = [word for word in tokenized_text if word.isalnum()]\n",
        "\n",
        "  # Eliminamos stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenized_text = [word for word in tokenized_text if word not in stop_words]\n",
        "\n",
        "  # Lemmatización\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokenized_text = [lemmatizer.lemmatize(word).strip() for word in tokenized_text]\n",
        "\n",
        "  # Convertimos números a palabras\n",
        "  for i, word in enumerate(tokenized_text):\n",
        "    if word.isdigit():\n",
        "      tokenized_text[i] = num2words(int(word), lang='en')\n",
        "\n",
        "  # Vamos a eliminar también las palabras más frecuentes, ya que no nos aportarán valor. Son palabras como vimos en el EDA como 'product', 'taste, 'flavour', que se usan en contexto\n",
        "  # tanto positivo como negativo\n",
        "  word_freq = FreqDist(tokenized_text)\n",
        "\n",
        "  most_common_words = set(word for word, freq in word_freq.most_common(5)) # Tras hacer pruebas, parece que eliminar las 5 más usadas da resultados razonables\n",
        "  #tokenized_text = [word for word in tokenized_text if word not in most_common_words]\n",
        "\n",
        "  # Unimos el resultado en un texto limpio\n",
        "  clean_text = ' '.join(tokenized_text)\n",
        "\n",
        "  return clean_text\n"
      ],
      "metadata": {
        "id": "Of2ehORiEz84"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Aplicación del preprocesado al corpus**"
      ],
      "metadata": {
        "id": "lVRUBq0QQ9Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_balanced_sent['processed_reviews'] = df_balanced_sent['reviewText'].apply(preprocess)"
      ],
      "metadata": {
        "id": "bibrABcgJI9E"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Review original: {df_balanced_sent['reviewText'].values[0]}\")\n",
        "print(f\"Review procesada: {df_balanced_sent['processed_reviews'].values[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1BRqlbNN4H1",
        "outputId": "1dc28deb-ff42-4d3a-99d4-c8ce91ba711f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review original: It's got a lot more kick than what you'll find at your local grocery store.\n",
            "I'd give it a solid 5 stars if it had a bit less sugar (and fewer calories).\n",
            "I've yet to find a ginger beer as strong as the stuff I had as a kid, but this is the best I've found so far.\n",
            "It's nothing like ginger ale, I don't know what the other guy was smoking.\n",
            "Review procesada: got lot kick find local grocery store give solid five star bit le sugar fewer calorie yet find ginger beer strong stuff kid best found far nothing like ginger ale know guy smoking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el dataset con el nuevo estado para pasar al modelado.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_balanced_sent.to_csv('/content/drive/MyDrive/NLP/df_processed.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSGQVaOytOO-",
        "outputId": "463514e1-b293-455f-8af0-df1afcaa07ca"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.4 Conclusiones**"
      ],
      "metadata": {
        "id": "I47rgS4ARGPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos ido construyendo la **función de preprocesad**o, haciendo diferentes pruebas.\n",
        "\n",
        "El **proceso** a seguir ha sido:\n",
        "  - **Eliminar tildes y signos de puntuación**.\n",
        "  - **Tokenizar y dejar en minúscula**s (ya dejamos en minúsculas anteriormente para ver las nubes de palabras y explorar el texto pero por si acaso aplicamos nuevamente).\n",
        "  - **Eliminamos las stop words** según el diccionario de stopwords en **inglés** de nltk.\n",
        "  - Realizamos la **lemmatización** y **eliminamos espacios extra**.\n",
        "  - Convertimos los posibles **números a palabras**.\n",
        "  - Eliminar las palabras más frecuentes. Agregamos esto en una nueva iteración, ya que vemos que las reviews resultantes del procesado incluyen demasiadas palabras que aparentemente no aportan valor, como por ejemplo \"Product\". Decidimos eliminar las 5 más comunes (ver nubes de palabras y otras representaciones en el apartado de EDA para obtener más información al respecto)\n",
        "  "
      ],
      "metadata": {
        "id": "FFBjA1b1t0t5"
      }
    }
  ]
}